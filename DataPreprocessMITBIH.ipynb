{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c3d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d90f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved MIT-BIH data grouped by subject to: ./mitbih_all_data.pt\n"
     ]
    }
   ],
   "source": [
    "def standardization(data):\n",
    "    mu = np.mean(data)\n",
    "    sigma = np.std(data)\n",
    "    return (data - mu) / sigma\n",
    "\n",
    "SAVE_PATH = './datasets/mitbih_all_data.pt'\n",
    "DATASET_PATH = './mit-bih'\n",
    "CLASSES = ['NSR', 'AFIB', 'APB', 'PVC', 'SDHB']\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.array(CLASSES)\n",
    "\n",
    "subject_data = {}\n",
    "\n",
    "for root, _, files in os.walk(DATASET_PATH):\n",
    "    for fname in files:\n",
    "        if not fname.endswith('.mat'):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Get class from subfolder name like \"0_NSR\"\n",
    "            folder_name = os.path.basename(root)\n",
    "            class_index = int(folder_name.split(' ')[0])-1\n",
    "            label_text = CLASSES[class_index]\n",
    "\n",
    "            fpath = os.path.join(root, fname)\n",
    "            mat = scio.loadmat(fpath)\n",
    "            signal = mat.get('val')\n",
    "            signal = signal[0]\n",
    "\n",
    "            # Downsample to 2500\n",
    "            indices = np.linspace(0, len(signal) - 1, 2500, dtype=int)\n",
    "            signal = signal[indices]\n",
    "            signal = standardization(signal)\n",
    "\n",
    "            # Subject ID from filename\n",
    "            subject_id = int(fname[:3])\n",
    "            label_idx = le.transform([label_text])[0]\n",
    "\n",
    "            if subject_id not in subject_data:\n",
    "                subject_data[subject_id] = {'x': [], 'y': []}\n",
    "\n",
    "            subject_data[subject_id]['x'].append(torch.tensor(signal, dtype=torch.float32))\n",
    "            subject_data[subject_id]['y'].append(torch.tensor(label_idx))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {fname}: {e}\")\n",
    "\n",
    "# Convert each subject's list of x/y to tensor stacks\n",
    "for sid in subject_data:\n",
    "    subject_data[sid]['x'] = torch.stack(subject_data[sid]['x'])\n",
    "    subject_data[sid]['y'] = torch.stack(subject_data[sid]['y'])\n",
    "\n",
    "torch.save({\n",
    "    'data_by_subject': subject_data,\n",
    "    'label_encoder': le\n",
    "}, SAVE_PATH)\n",
    "\n",
    "print(f\"Saved MIT-BIH data grouped by subject to: {SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dafbc793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NSR' 'AFIB' 'APB' 'PVC' 'SDHB']\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be020155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Union, List, Optional\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d59bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MITBIH_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for ECG samples grouped by subject (MIT-BIH).\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the .pt file with data grouped by subject.\n",
    "        normal (bool or None): If True, include only normal (label == 0); if False, only abnormal; if None, include all.\n",
    "        subject_ids (Union[float, List[float], None]): Subject(s) to include. If None, uses all subjects.\n",
    "        split (str): 'train', 'test', or None â€” whether to return a subset.\n",
    "        test_ratio (float): Proportion to reserve for test split.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        normal: Optional[bool] = None,\n",
    "        subject_ids: Optional[Union[float, List[float]]] = None,\n",
    "        split: Optional[str] = None,\n",
    "        test_ratio: float = 0.7,\n",
    "        random_seed: int = 42\n",
    "    ):\n",
    "        assert split in [None, 'train', 'test'], \"split must be None, 'train', or 'test'\"\n",
    "\n",
    "        raw_data = torch.load(data_path)\n",
    "        all_subject_data = raw_data['data_by_subject']\n",
    "        self.label_encoder = raw_data['label_encoder']\n",
    "\n",
    "        # Normalize subject_ids\n",
    "        if subject_ids is None:\n",
    "            selected_subjects = list(all_subject_data.keys())\n",
    "        elif isinstance(subject_ids, float):\n",
    "            selected_subjects = [subject_ids]\n",
    "        else:\n",
    "            selected_subjects = subject_ids\n",
    "\n",
    "        # Collect and filter samples\n",
    "        all_samples = []\n",
    "        for sid in selected_subjects:\n",
    "            subject_data = all_subject_data[sid]\n",
    "            for x, y in zip(subject_data['x'], subject_data['y']):\n",
    "                y_int = int(y.item())\n",
    "                if normal is True and y_int != 0:\n",
    "                    continue  # keep only label==0\n",
    "                if normal is False and y_int == 0:\n",
    "                    continue  # exclude label==0\n",
    "                if normal is False:\n",
    "                    y_int -= 1  # shift all labels by -1 so they start from 0\n",
    "\n",
    "                all_samples.append((x, torch.tensor(y_int)))\n",
    "\n",
    "        if len(all_samples) == 0:\n",
    "            raise ValueError(\"No samples in the dataset. Check filtering conditions.\")\n",
    "\n",
    "        # Optional split\n",
    "        if split is not None:\n",
    "            stratify_labels = [int(y) for _, y in all_samples]\n",
    "            train_idx, test_idx = train_test_split(\n",
    "                range(len(all_samples)),\n",
    "                test_size=test_ratio,\n",
    "                random_state=random_seed,\n",
    "                stratify=stratify_labels\n",
    "            )\n",
    "            indices = train_idx if split == 'train' else test_idx\n",
    "            self.samples = [all_samples[i] for i in indices]\n",
    "        else:\n",
    "            self.samples = all_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        return x.unsqueeze(0), y  # Add channel dimension (1, 2500)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
