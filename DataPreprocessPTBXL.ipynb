{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a05265c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import wfdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd3a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and grouping ECGs by subject...\n",
      "Saved PTBXL data grouped by subject to: ./ptbxl_normal_data.pt\n"
     ]
    }
   ],
   "source": [
    "# === CONFIG ===\n",
    "BASE_PATH = \"./ptb-xl/\"\n",
    "CSV_PATH = os.path.join(BASE_PATH, \"ptbxl_database.csv\")\n",
    "SAVE_PATH = \"./datasets/ptbxl_all_data.pt\"\n",
    "SEGMENT_LENGTH = 2500  # 10 seconds at 250 Hz\n",
    "TARGET_LABELS = ['NORM', 'AFIB', 'PAC', 'PVC', 'SBRAD', 'STACH']\n",
    "ARTIFACT_COLUMNS = ['baseline_drift', 'static_noise', 'burst_noise', 'electrodes_problems']\n",
    "\n",
    "# === LOAD METADATA ===\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df[df['ecg_id'].notna()]\n",
    "df['scp_codes'] = df['scp_codes'].apply(eval)\n",
    "\n",
    "# === Load and filter valid diagnostic SCP codes ===\n",
    "scp_statements = pd.read_csv(os.path.join(BASE_PATH, 'scp_statements.csv'), index_col=0)\n",
    "diagnostic_classes = list(scp_statements.index)\n",
    "\n",
    "# === Select highest-confidence SCP code among TARGET_LABELS\n",
    "def select_top_label(code_dict):\n",
    "    filtered = {k: v for k, v in code_dict.items() if k in diagnostic_classes and k in TARGET_LABELS}\n",
    "    if not filtered:\n",
    "        return None\n",
    "    return max(filtered, key=filtered.get)\n",
    "\n",
    "df['label'] = df['scp_codes'].map(select_top_label)\n",
    "df = df[df['label'].notna()]\n",
    "\n",
    "# === Remove rows where artifacts in Lead II are noted\n",
    "for col in ARTIFACT_COLUMNS:\n",
    "    df = df[~df[col].fillna('').str.contains(r'\\bII\\b', regex=True)]\n",
    "\n",
    "# === Encode final labels\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.array(TARGET_LABELS)\n",
    "df['label_idx'] = le.transform(df['label'])\n",
    "\n",
    "# === Extract ECG Lead II and group by subject\n",
    "subject_data = {}\n",
    "print(\"Extracting and grouping ECGs by subject...\")\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    patient_id = row['patient_id']\n",
    "    file_path = os.path.join(BASE_PATH, row['filename_hr'])\n",
    "\n",
    "    try:\n",
    "        record = wfdb.rdrecord(file_path)\n",
    "        ecg = record.p_signal.T  # shape: (12, N_samples)\n",
    "        lead2 = ecg[1] # LEAD II\n",
    "        downsampled = lead2[::2]  # 500 Hz → 250 Hz\n",
    "        normalized = (downsampled - np.mean(downsampled)) / np.std(downsampled)\n",
    "\n",
    "        x_tensor = torch.tensor(normalized, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(row['label_idx'])\n",
    "\n",
    "        if patient_id not in subject_data:\n",
    "            subject_data[patient_id] = {'x': [], 'y': []}\n",
    "\n",
    "        subject_data[patient_id]['x'].append(x_tensor)\n",
    "        subject_data[patient_id]['y'].append(y_tensor)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {row['filename_hr']} (patient {patient_id}): {e}\")\n",
    "\n",
    "# === Convert lists to tensors\n",
    "for pid in subject_data:\n",
    "    subject_data[pid]['x'] = torch.stack(subject_data[pid]['x'])\n",
    "    subject_data[pid]['y'] = torch.stack(subject_data[pid]['y'])\n",
    "\n",
    "# === Save dataset\n",
    "torch.save({\n",
    "    'data_by_subject': subject_data,\n",
    "    'label_encoder': le\n",
    "}, SAVE_PATH)\n",
    "\n",
    "print(f\"Saved PTBXL data grouped by subject to: {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "732e1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Union, List, Optional\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PTBXL_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for ECG samples grouped by subject (PTB-XL).\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the .pt file with data grouped by subject.\n",
    "        subject_ids (Union[float, List[float], None]): Subject(s) to include. If None, uses all subjects.\n",
    "        split (str): 'train', 'test', or None — whether to return a subset.\n",
    "        test_ratio (float): Proportion to reserve for test split (if split is specified).\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        subject_ids: Optional[Union[float, List[float]]] = None,\n",
    "        split: Optional[str] = None,\n",
    "        test_ratio: float = 0.2,\n",
    "        val_ratio: float = 0.2,\n",
    "        random_seed: int = 42\n",
    "    ):\n",
    "        assert split in [None, 'train', 'val', 'test'], \"split must be None, 'train', 'val', or 'test'\"\n",
    "\n",
    "        raw_data = torch.load(data_path)\n",
    "        all_subject_data = raw_data['data_by_subject']\n",
    "        self.label_encoder = raw_data['label_encoder']\n",
    "        # label index: [0:'AFIB' 1:'NORM' 2:'PAC' 3:'PVC' 4:'SBRAD' 5:'STACH']\n",
    "\n",
    "        # Normalize subject_ids to list\n",
    "        if subject_ids is None:\n",
    "            selected_subjects = list(all_subject_data.keys())\n",
    "        elif isinstance(subject_ids, float):\n",
    "            selected_subjects = [subject_ids]\n",
    "        else:\n",
    "            selected_subjects = subject_ids\n",
    "\n",
    "        # Collect all (x, y) pairs per subject\n",
    "        all_samples = []\n",
    "\n",
    "        for sid in selected_subjects:\n",
    "            subject_data = all_subject_data[sid]\n",
    "            x_list = subject_data['x']\n",
    "            y_list = subject_data['y']\n",
    "            samples = list(zip(x_list, y_list))\n",
    "            all_samples.extend(samples)\n",
    "\n",
    "        # Split based on sample count\n",
    "        if split is not None:\n",
    "            stratify_labels = [int(y) for _, y in all_samples]\n",
    "\n",
    "            # First split into temp (train+val) and test\n",
    "            temp_idx, test_idx = train_test_split(\n",
    "                range(len(all_samples)),\n",
    "                test_size=test_ratio,\n",
    "                random_state=random_seed,\n",
    "                stratify=stratify_labels\n",
    "            )\n",
    "\n",
    "            temp_samples = [all_samples[i] for i in temp_idx]\n",
    "            temp_labels = [int(y) for _, y in temp_samples]\n",
    "\n",
    "            # Now split temp into train and val\n",
    "            train_idx, val_idx = train_test_split(\n",
    "                range(len(temp_samples)),\n",
    "                test_size=val_ratio,\n",
    "                random_state=random_seed,\n",
    "                stratify=temp_labels\n",
    "            )\n",
    "\n",
    "            if split == 'train':\n",
    "                indices = [temp_idx[i] for i in train_idx]\n",
    "            elif split == 'val':\n",
    "                indices = [temp_idx[i] for i in val_idx]\n",
    "            elif split == 'test':\n",
    "                indices = test_idx\n",
    "\n",
    "            self.samples = [all_samples[i] for i in indices]\n",
    "        else:\n",
    "            self.samples = all_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        return x.unsqueeze(0), y  # Add channel dimension: (1, 2500)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
